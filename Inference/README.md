# Inference : Self-Study-On-SageMaker

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2022.11.26**


---

# 0. ì„¸ì´ì§€ ë©”ì´ì»¤ ë°°í¬ ë° ì¶”ë¡  ì…ë¬¸
- ì „ë°˜ì ì¸ ì¶”ë¡  ê³¼ì •ì˜ ì „ì²´ì™€ ì•„í‚¤í…ì²˜ë¥¼ ì˜ ì„¤ëª… í•©ë‹ˆë‹¤.
    - [High-Performance & Cost-Effective Model Deployment Strategies with Amazon SageMaker](https://www.youtube.com/watch?v=_-hX-2MqiOg)

# 1. ì›Œí¬ìƒµ ìë£Œ
## 1.1 ëª¨ë¸ ì„œë¹™ í˜í„´  ê°€ì´ë“œ

* ë‹¤ì–‘í•œ ëª¨ë¸ ì„œë¹™ íŒ¨í„´, A/B í…ŒìŠ¤íŒ… ì— ëŒ€í•œ ì„¤ëª… ë° í•¸ì¦ˆì˜¨ ìë£Œ ë§í¬ê°€ ìˆìŠµë‹ˆë‹¤.
    * https://main.dczm1kv9dpvdi.amplifyapp.com/ko/
    
    
# 2. ì¶”ì²œ ë¸”ë¡œê·¸
- (Nov 2022, ìˆ˜ì¤€: ì¤‘ê¸‰) [Getting started with deploying real-time models on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/getting-started-with-deploying-real-time-models-on-amazon-sagemaker/) 
    - ì „ì²´ì ì¸ SageMaker ì˜ ì¶”ë¡  ì˜µì…˜ì„ ì„¤ëª…ì„ í•˜ê³  ìˆìŒ. 
    - ë˜í•œ í”„ë¡œê·¸ë˜ë° ê´€ì ì—ì„œ  SageMaker Python SDK ì™€ AWS SDK/Boto3 ì˜ ì°¨ì´ì ì„ ê¸°ìˆ  í•¨.
        - SageMaker Python SDK
            - ë¹ ë¥´ê²Œ ì‹¤í—˜ì˜ ìš©ë„ë¡œ ì‚¬ìš©. Estimator.deploy() ë§Œìœ¼ë¡œ ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
        - AWS SDK/Boto3
            - í”„ë¡œë•ì…˜ ìš©ìœ¼ë¡œ ì‚¬ìš©ì„ ê¶Œì¥í•˜ë©´, Model Creation, Endpoint Configuration Creation, and Endpoint Creation ì„¸ê°œì˜ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§.            
    - ![model-deployment-options.png](img/model-deployment-options.png)
- (Nov 2022, ìˆ˜ì¤€: ì¤‘ê¸‰) [Model Hosting Patterns in SageMaker: Best practices in testing and updating models on SageMaker](https://aws.amazon.com/blogs/machine-learning/model-hosting-patterns-in-sagemaker-best-practices-in-testing-and-updating-models-on-sagemaker/)
    - ì„¸ì´ì§€ ë©”ì´ì»¤ì—ì„œ Real time Endpoint ë¥¼ Model Hosting Pattern ìœ¼ë¡œ ì •ë¦¬í•œ ë¸”ë¡œê·¸ ì„. ì´ ì¤‘ì—ì„œ Multi-Variant Endpoint ì— ëŒ€í•œ ì¥/ë‹¨ì , AB í…ŒìŠ¤íŒ… ë°  ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ì˜ ì„¤ëª… í•¨. 
    - ![real-time-inference.jpeg](img/real-time-inference.jpeg)
- (Nov 2022, ìˆ˜ì¤€: ê³ ê¸‰) [Serve multiple models with Amazon SageMaker and Triton Inference Server](https://aws.amazon.com/blogs/machine-learning/serve-multiple-models-with-amazon-sagemaker-and-triton-inference-server/)
    - í•˜ë‚˜ì˜ SageMaker Endpoint ì—ì„œ NVIDIA Triton Inference Server ì— 3ê°œì˜ ëª¨ë¸ì„ ë™ì‹œì— ì„œë¹™í•˜ëŠ” ì˜ˆì‹œ ì„.
    - ![triton-server.png](img/triton-server.png)
    
    
# 3. í—ˆê¹… í˜ì´ìŠ¤ ëª¨ë¸ ë°°í¬ ë° ì¶”ë¡ 

## 3.1. ê³µì‹ íŠœí† ë¦¬ì–¼
- ì•„ë˜ í—ˆê¹…í˜ì´ìŠ¤ ê³µì‹ ìë£ŒëŠ” ì•„ë˜ì™€ ê°™ì€ ë‚´ìš©ì„ ì†Œê°œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    - Deploy a ğŸ¤— Transformers model trained in SageMaker.
    - Deploy a ğŸ¤— Transformers model from the Hugging Face 
    - Run a Batch Transform Job using ğŸ¤— Transformers and Amazon SageMaker.
    - Create a custom inference module.
- [Deploy models to Amazon SageMaker](https://huggingface.co/docs/sagemaker/inference)
   